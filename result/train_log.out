INFO: COMMAND: train.py --save-dir /home/lvyajie/nlu_cw2/result/Q7 --log-file /home/lvyajie/nlu_cw2/result/Q7/log.txt --data /home/lvyajie/nlu_cw2/europarl_prepared --cuda True --arch transformer
INFO: Arguments: {'activation_dropout': 0.4,
 'arch': 'transformer',
 'attention_dropout': 0.4,
 'batch_size': 10,
 'clip_norm': 4.0,
 'cuda': 'True',
 'data': '/home/lvyajie/nlu_cw2/europarl_prepared',
 'decoder_attention_heads': 2,
 'decoder_embed_dim': 128,
 'decoder_ffn_embed_dim': 512,
 'decoder_layers': 2,
 'device_id': 0,
 'dropout': 0.4,
 'encoder_attention_heads': 2,
 'encoder_embed_dim': 128,
 'encoder_ffn_embed_dim': 512,
 'encoder_layers': 2,
 'epoch_checkpoints': False,
 'log_file': '/home/lvyajie/nlu_cw2/result/Q7/log.txt',
 'lr': 0.0003,
 'max_epoch': 100,
 'max_tokens': None,
 'no_save': False,
 'no_scale_embedding': False,
 'patience': 10,
 'restore_file': 'checkpoint_last.pt',
 'save_dir': '/home/lvyajie/nlu_cw2/result/Q7',
 'save_interval': 1,
 'source_lang': 'de',
 'target_lang': 'en',
 'train_on_tiny': False}
INFO: Loaded a source dictionary (de) with 5047 words
INFO: Loaded a target dictionary (en) with 4420 words
INFO: Built a model with 2707652 parameters
INFO: Epoch 000: loss 5.519 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 32.07 | clip 1
INFO: Epoch 000: valid_loss 4.71 | num_tokens 13.8 | batch_size 500 | valid_perplexity 111
INFO: Epoch 001: loss 4.674 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 28.13 | clip 1
INFO: Epoch 001: valid_loss 4.23 | num_tokens 13.8 | batch_size 500 | valid_perplexity 68.8
INFO: Epoch 002: loss 4.27 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 28.73 | clip 1
INFO: Epoch 002: valid_loss 3.97 | num_tokens 13.8 | batch_size 500 | valid_perplexity 52.8
INFO: Epoch 003: loss 3.994 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 28.6 | clip 1
INFO: Epoch 003: valid_loss 3.77 | num_tokens 13.8 | batch_size 500 | valid_perplexity 43.6
INFO: Epoch 004: loss 3.781 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 29.15 | clip 1
INFO: Epoch 004: valid_loss 3.64 | num_tokens 13.8 | batch_size 500 | valid_perplexity 38.2
INFO: Epoch 005: loss 3.601 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 30.88 | clip 1
INFO: Epoch 005: valid_loss 3.52 | num_tokens 13.8 | batch_size 500 | valid_perplexity 33.9
INFO: Epoch 006: loss 3.448 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 29.98 | clip 1
INFO: Epoch 006: valid_loss 3.43 | num_tokens 13.8 | batch_size 500 | valid_perplexity 31
INFO: Epoch 007: loss 3.307 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 34.21 | clip 1
INFO: Epoch 007: valid_loss 3.36 | num_tokens 13.8 | batch_size 500 | valid_perplexity 28.7
INFO: Epoch 008: loss 3.178 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 30.91 | clip 1
INFO: Epoch 008: valid_loss 3.3 | num_tokens 13.8 | batch_size 500 | valid_perplexity 27.1
INFO: Epoch 009: loss 3.061 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 32.84 | clip 1
INFO: Epoch 009: valid_loss 3.23 | num_tokens 13.8 | batch_size 500 | valid_perplexity 25.4
INFO: Epoch 010: loss 2.955 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 33.04 | clip 1
INFO: Epoch 010: valid_loss 3.2 | num_tokens 13.8 | batch_size 500 | valid_perplexity 24.5
INFO: Epoch 011: loss 2.846 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 37.51 | clip 1
INFO: Epoch 011: valid_loss 3.16 | num_tokens 13.8 | batch_size 500 | valid_perplexity 23.5
INFO: Epoch 012: loss 2.752 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 33.84 | clip 1
INFO: Epoch 012: valid_loss 3.12 | num_tokens 13.8 | batch_size 500 | valid_perplexity 22.7
INFO: Epoch 013: loss 2.66 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 34.86 | clip 1
INFO: Epoch 013: valid_loss 3.1 | num_tokens 13.8 | batch_size 500 | valid_perplexity 22.2
INFO: Epoch 014: loss 2.579 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 35.7 | clip 1
INFO: Epoch 014: valid_loss 3.08 | num_tokens 13.8 | batch_size 500 | valid_perplexity 21.7
INFO: Epoch 015: loss 2.495 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 36.59 | clip 1
INFO: Epoch 015: valid_loss 3.05 | num_tokens 13.8 | batch_size 500 | valid_perplexity 21.2
INFO: Epoch 016: loss 2.415 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 37.2 | clip 1
INFO: Epoch 016: valid_loss 3.03 | num_tokens 13.8 | batch_size 500 | valid_perplexity 20.7
INFO: Epoch 017: loss 2.341 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 37.89 | clip 1
INFO: Epoch 017: valid_loss 3.02 | num_tokens 13.8 | batch_size 500 | valid_perplexity 20.4
INFO: Epoch 018: loss 2.275 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 39.42 | clip 1
INFO: Epoch 018: valid_loss 2.99 | num_tokens 13.8 | batch_size 500 | valid_perplexity 20
INFO: Epoch 019: loss 2.214 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 39.11 | clip 1
INFO: Epoch 019: valid_loss 2.98 | num_tokens 13.8 | batch_size 500 | valid_perplexity 19.7
INFO: Epoch 020: loss 2.144 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 39.52 | clip 1
INFO: Epoch 020: valid_loss 2.96 | num_tokens 13.8 | batch_size 500 | valid_perplexity 19.3
INFO: Epoch 021: loss 2.085 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 39.93 | clip 1
INFO: Epoch 021: valid_loss 2.96 | num_tokens 13.8 | batch_size 500 | valid_perplexity 19.3
INFO: Epoch 022: loss 2.033 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 41.66 | clip 1
INFO: Epoch 022: valid_loss 2.95 | num_tokens 13.8 | batch_size 500 | valid_perplexity 19
INFO: Epoch 023: loss 1.977 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 44.78 | clip 1
INFO: Epoch 023: valid_loss 2.94 | num_tokens 13.8 | batch_size 500 | valid_perplexity 18.8
INFO: Epoch 024: loss 1.93 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 40.61 | clip 1
INFO: Epoch 024: valid_loss 2.92 | num_tokens 13.8 | batch_size 500 | valid_perplexity 18.6
INFO: Epoch 025: loss 1.88 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 41.54 | clip 1
INFO: Epoch 025: valid_loss 2.92 | num_tokens 13.8 | batch_size 500 | valid_perplexity 18.6
INFO: Epoch 026: loss 1.838 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 46.03 | clip 1
INFO: Epoch 026: valid_loss 2.9 | num_tokens 13.8 | batch_size 500 | valid_perplexity 18.2
INFO: Epoch 027: loss 1.793 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 41.5 | clip 1
INFO: Epoch 027: valid_loss 2.89 | num_tokens 13.8 | batch_size 500 | valid_perplexity 18
INFO: Epoch 028: loss 1.754 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 41.52 | clip 1
INFO: Epoch 028: valid_loss 2.9 | num_tokens 13.8 | batch_size 500 | valid_perplexity 18.2
INFO: Epoch 029: loss 1.719 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 45.63 | clip 1
INFO: Epoch 029: valid_loss 2.89 | num_tokens 13.8 | batch_size 500 | valid_perplexity 18
INFO: Epoch 030: loss 1.681 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 41.84 | clip 1
INFO: Epoch 030: valid_loss 2.87 | num_tokens 13.8 | batch_size 500 | valid_perplexity 17.6
INFO: Epoch 031: loss 1.652 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 42.54 | clip 1
INFO: Epoch 031: valid_loss 2.86 | num_tokens 13.8 | batch_size 500 | valid_perplexity 17.5
INFO: Epoch 032: loss 1.624 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 42.59 | clip 1
INFO: Epoch 032: valid_loss 2.87 | num_tokens 13.8 | batch_size 500 | valid_perplexity 17.7
INFO: Epoch 033: loss 1.591 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 42.77 | clip 1
INFO: Epoch 033: valid_loss 2.87 | num_tokens 13.8 | batch_size 500 | valid_perplexity 17.6
INFO: Epoch 034: loss 1.564 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 43.04 | clip 1
INFO: Epoch 034: valid_loss 2.86 | num_tokens 13.8 | batch_size 500 | valid_perplexity 17.5
INFO: Epoch 035: loss 1.539 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 42.63 | clip 1
INFO: Epoch 035: valid_loss 2.87 | num_tokens 13.8 | batch_size 500 | valid_perplexity 17.7
INFO: Epoch 036: loss 1.518 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 42.98 | clip 1
INFO: Epoch 036: valid_loss 2.86 | num_tokens 13.8 | batch_size 500 | valid_perplexity 17.5
INFO: Epoch 037: loss 1.497 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 42.95 | clip 1
INFO: Epoch 037: valid_loss 2.86 | num_tokens 13.8 | batch_size 500 | valid_perplexity 17.4
INFO: Epoch 038: loss 1.476 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 43.85 | clip 1
INFO: Epoch 038: valid_loss 2.87 | num_tokens 13.8 | batch_size 500 | valid_perplexity 17.7
INFO: Epoch 039: loss nan | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm nan | clip 0.82
INFO: Epoch 039: valid_loss nan | num_tokens 13.8 | batch_size 500 | valid_perplexity nan
INFO: Epoch 040: loss nan | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm nan | clip 0
INFO: Epoch 040: valid_loss nan | num_tokens 13.8 | batch_size 500 | valid_perplexity nan
INFO: Epoch 041: loss nan | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm nan | clip 0
INFO: Epoch 041: valid_loss nan | num_tokens 13.8 | batch_size 500 | valid_perplexity nan
INFO: Epoch 042: loss nan | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm nan | clip 0
INFO: Epoch 042: valid_loss nan | num_tokens 13.8 | batch_size 500 | valid_perplexity nan
INFO: Epoch 043: loss nan | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm nan | clip 0
INFO: Epoch 043: valid_loss nan | num_tokens 13.8 | batch_size 500 | valid_perplexity nan
INFO: Epoch 044: loss nan | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm nan | clip 0
INFO: Epoch 044: valid_loss nan | num_tokens 13.8 | batch_size 500 | valid_perplexity nan
INFO: Epoch 045: loss nan | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm nan | clip 0
INFO: Epoch 045: valid_loss nan | num_tokens 13.8 | batch_size 500 | valid_perplexity nan
INFO: Epoch 046: loss nan | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm nan | clip 0
INFO: Epoch 046: valid_loss nan | num_tokens 13.8 | batch_size 500 | valid_perplexity nan
INFO: Epoch 047: loss nan | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm nan | clip 0
INFO: Epoch 047: valid_loss nan | num_tokens 13.8 | batch_size 500 | valid_perplexity nan
INFO: No validation set improvements observed for 10 epochs. Early stop!
