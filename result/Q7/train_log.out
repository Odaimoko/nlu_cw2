INFO: COMMAND: train.py --save-dir /home/lvyajie/nlu_cw2/result/Q7 --log-file /home/lvyajie/nlu_cw2/result/Q7/log.txt --data /home/lvyajie/nlu_cw2/europarl_prepared --cuda True --arch transformer
INFO: Arguments: {'activation_dropout': 0.4,
 'arch': 'transformer',
 'attention_dropout': 0.4,
 'batch_size': 10,
 'clip_norm': 4.0,
 'cuda': 'True',
 'data': '/home/lvyajie/nlu_cw2/europarl_prepared',
 'decoder_attention_heads': 2,
 'decoder_embed_dim': 128,
 'decoder_ffn_embed_dim': 512,
 'decoder_layers': 2,
 'device_id': 0,
 'dropout': 0.4,
 'encoder_attention_heads': 2,
 'encoder_embed_dim': 128,
 'encoder_ffn_embed_dim': 512,
 'encoder_layers': 2,
 'epoch_checkpoints': False,
 'log_file': '/home/lvyajie/nlu_cw2/result/Q7/log.txt',
 'lr': 0.0003,
 'max_epoch': 100,
 'max_tokens': None,
 'no_save': False,
 'no_scale_embedding': False,
 'patience': 10,
 'restore_file': 'checkpoint_last.pt',
 'save_dir': '/home/lvyajie/nlu_cw2/result/Q7',
 'save_interval': 1,
 'source_lang': 'de',
 'target_lang': 'en',
 'train_on_tiny': False}
INFO: Loaded a source dictionary (de) with 5047 words
INFO: Loaded a target dictionary (en) with 4420 words
INFO: Built a model with 2707652 parameters
INFO: Epoch 000: loss 5.355 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 28.64 | clip 1
INFO: Epoch 000: valid_loss 4.3 | num_tokens 13.8 | batch_size 500 | valid_perplexity 73.5
INFO: Epoch 001: loss 4.267 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 36.03 | clip 1
INFO: Epoch 001: valid_loss 3.43 | num_tokens 13.8 | batch_size 500 | valid_perplexity 30.9
INFO: Epoch 002: loss 3.627 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 41.8 | clip 1
INFO: Epoch 002: valid_loss 2.78 | num_tokens 13.8 | batch_size 500 | valid_perplexity 16.1
INFO: Epoch 003: loss 3.114 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 46.44 | clip 1
INFO: Epoch 003: valid_loss 2.23 | num_tokens 13.8 | batch_size 500 | valid_perplexity 9.32
INFO: Epoch 004: loss 2.674 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 50.44 | clip 1
INFO: Epoch 004: valid_loss 1.76 | num_tokens 13.8 | batch_size 500 | valid_perplexity 5.82
INFO: Epoch 005: loss 2.311 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 54.22 | clip 1
INFO: Epoch 005: valid_loss 1.38 | num_tokens 13.8 | batch_size 500 | valid_perplexity 3.98
INFO: Epoch 006: loss 2.005 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 57.33 | clip 1
INFO: Epoch 006: valid_loss 1.11 | num_tokens 13.8 | batch_size 500 | valid_perplexity 3.03
INFO: Epoch 007: loss 1.749 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 59.94 | clip 1
INFO: Epoch 007: valid_loss 0.885 | num_tokens 13.8 | batch_size 500 | valid_perplexity 2.42
INFO: Epoch 008: loss 1.527 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 61.83 | clip 1
INFO: Epoch 008: valid_loss 0.715 | num_tokens 13.8 | batch_size 500 | valid_perplexity 2.05
INFO: Epoch 009: loss 1.352 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 63.2 | clip 1
INFO: Epoch 009: valid_loss 0.579 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.78
INFO: Epoch 010: loss 1.196 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 64.67 | clip 1
INFO: Epoch 010: valid_loss 0.485 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.62
INFO: Epoch 011: loss 1.073 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 65.2 | clip 1
INFO: Epoch 011: valid_loss 0.452 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.57
INFO: Epoch 012: loss 0.9687 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 66.03 | clip 1
INFO: Epoch 012: valid_loss 0.363 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.44
INFO: Epoch 013: loss 0.8761 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 66.66 | clip 1
INFO: Epoch 013: valid_loss 0.317 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.37
INFO: Epoch 014: loss 0.8038 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 65.56 | clip 1
INFO: Epoch 014: valid_loss 0.3 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.35
INFO: Epoch 015: loss 0.73 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 66.27 | clip 0.999
INFO: Epoch 015: valid_loss 0.243 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.28
INFO: Epoch 016: loss 0.6808 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 66.11 | clip 0.999
INFO: Epoch 016: valid_loss 0.219 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.25
INFO: Epoch 017: loss 0.6368 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 65.72 | clip 1
INFO: Epoch 017: valid_loss 0.205 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.23
INFO: Epoch 018: loss 0.5938 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 66.02 | clip 0.998
INFO: Epoch 018: valid_loss 0.222 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.25
INFO: Epoch 019: loss 0.568 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 65.42 | clip 0.998
INFO: Epoch 019: valid_loss 0.206 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.23
INFO: Epoch 020: loss 0.5291 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 64.84 | clip 1
INFO: Epoch 020: valid_loss 0.205 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.23
INFO: Epoch 021: loss 0.5028 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 64.19 | clip 1
INFO: Epoch 021: valid_loss 0.165 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.18
INFO: Epoch 022: loss 0.4852 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 64.06 | clip 1
INFO: Epoch 022: valid_loss 0.147 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.16
INFO: Epoch 023: loss 0.4648 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 64.11 | clip 0.999
INFO: Epoch 023: valid_loss 0.156 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.17
INFO: Epoch 024: loss 0.4381 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 63.12 | clip 0.999
INFO: Epoch 024: valid_loss 0.134 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.14
INFO: Epoch 025: loss 0.4283 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 62.88 | clip 0.998
INFO: Epoch 025: valid_loss 0.168 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.18
INFO: Epoch 026: loss 0.4143 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 62.34 | clip 0.997
INFO: Epoch 026: valid_loss 0.128 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.14
INFO: Epoch 027: loss 0.4032 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 62.2 | clip 0.999
INFO: Epoch 027: valid_loss 0.143 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.15
INFO: Epoch 028: loss 0.393 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 62.29 | clip 0.998
INFO: Epoch 028: valid_loss 0.163 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.18
INFO: Epoch 029: loss 0.3787 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 61.28 | clip 0.998
INFO: Epoch 029: valid_loss 0.149 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.16
INFO: Epoch 030: loss 0.3702 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 60.52 | clip 0.998
INFO: Epoch 030: valid_loss 0.135 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.14
INFO: Epoch 031: loss 0.3561 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 59.55 | clip 0.999
INFO: Epoch 031: valid_loss 0.144 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.15
INFO: Epoch 032: loss 0.3509 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 59.54 | clip 0.998
INFO: Epoch 032: valid_loss 0.146 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.16
INFO: Epoch 033: loss 0.3406 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 59.87 | clip 0.996
INFO: Epoch 033: valid_loss 0.127 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.14
INFO: Epoch 034: loss 0.3365 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 59.11 | clip 0.996
INFO: Epoch 034: valid_loss 0.141 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.15
INFO: Epoch 035: loss 0.3247 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 59.07 | clip 0.997
INFO: Epoch 035: valid_loss 0.136 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.15
INFO: Epoch 036: loss 0.3211 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 58.55 | clip 0.998
INFO: Epoch 036: valid_loss 0.119 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.13
INFO: Epoch 037: loss 0.3098 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 57.81 | clip 0.995
INFO: Epoch 037: valid_loss 0.133 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.14
INFO: Epoch 038: loss 0.3026 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 57.63 | clip 0.994
INFO: Epoch 038: valid_loss 0.126 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.13
INFO: Epoch 039: loss 0.3038 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 57.97 | clip 0.998
INFO: Epoch 039: valid_loss 0.12 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.13
INFO: Epoch 040: loss 0.2915 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 56.57 | clip 0.999
INFO: Epoch 040: valid_loss 0.118 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.13
INFO: Epoch 041: loss 0.2855 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 56.21 | clip 0.995
INFO: Epoch 041: valid_loss 0.115 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.12
INFO: Epoch 042: loss 0.2915 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 57.24 | clip 0.997
INFO: Epoch 042: valid_loss 0.0995 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.1
INFO: Epoch 043: loss 0.2813 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 56.61 | clip 0.997
INFO: Epoch 043: valid_loss 0.119 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.13
INFO: Epoch 044: loss 0.2786 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 55.64 | clip 0.995
INFO: Epoch 044: valid_loss 0.106 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.11
INFO: Epoch 045: loss 0.2655 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 54.78 | clip 0.995
INFO: Epoch 045: valid_loss 0.092 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.1
INFO: Epoch 046: loss 0.2621 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 55.21 | clip 0.996
INFO: Epoch 046: valid_loss 0.0955 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.1
INFO: Epoch 047: loss 0.249 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 54.12 | clip 0.995
INFO: Epoch 047: valid_loss 0.0996 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.1
INFO: Epoch 048: loss 0.2582 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 54.52 | clip 0.99
INFO: Epoch 048: valid_loss 0.121 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.13
INFO: Epoch 049: loss 0.2469 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 53.96 | clip 0.992
INFO: Epoch 049: valid_loss 0.103 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.11
INFO: Epoch 050: loss 0.2374 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 53.84 | clip 0.994
INFO: Epoch 050: valid_loss 0.105 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.11
INFO: Epoch 051: loss 0.2329 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 54.04 | clip 0.994
INFO: Epoch 051: valid_loss 0.0958 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.1
INFO: Epoch 052: loss 0.2341 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 53.3 | clip 0.994
INFO: Epoch 052: valid_loss 0.0954 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.1
INFO: Epoch 053: loss 0.2274 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 53.86 | clip 0.996
INFO: Epoch 053: valid_loss 0.0949 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.1
INFO: Epoch 054: loss 0.2162 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 51.88 | clip 0.993
INFO: Epoch 054: valid_loss 0.0928 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.1
INFO: Epoch 055: loss 0.2151 | lr 0.0003 | num_tokens 13.4 | batch_size 10 | grad_norm 52.61 | clip 0.988
INFO: Epoch 055: valid_loss 0.0994 | num_tokens 13.8 | batch_size 500 | valid_perplexity 1.1
INFO: No validation set improvements observed for 10 epochs. Early stop!
